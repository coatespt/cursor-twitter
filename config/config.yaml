# Sample configuration for the Twitter subject detection pipeline

# So far the only input mode is RabbitMQ
mode: mqj
mq_host: localhost
mq_port: 5672
mq_queue: tweet_in

# The number of tweets to keep in memory
# Also the set of tweets over which global frequency classes are computed
window: 1000000

# The number of tweets to process in each batch
batch: 5000

# Whether to log to the console
verbose: true 

# The directory to log to
log_dir: ../logs

# The minimum Z score for a 3pk element to be considered anomalous
z_score: 7

# The number of frequency classes to create
freq_classes: 7

# The length of the busy word processor counter arrays
bw_array_len: 1000

# List of frequency classes (1-based) to skip for busy word detection. E.g., [1, 2] skips the most frequent and second most frequent classes.
skip_frequency_classes: [1]

# Word filtering configuration
filter:
  # Enable word filtering
  enabled: true
  # Path to file containing words to filter out
  filter_file: "./config/filter_lists/test_filters.txt"

# Persistence configuration
persistence:
  # Directory for storing state files (counts, filters, 3pk mappings)
  state_dir: "../data/state"
  # Token batch size for file-based sliding window (tokens per file)
  token_batch_persist: 50000
  # Number of token files to keep before deleting oldest
  token_persist_files: 20

# Sender status tracking
sender:
  # File to track the last processed CSV file
  # NOTE: To future me--this is relative to where the sender runs from so 
  # it's actually next to the ../data/state directory. Confusing.
  status_file: "../../data/sender/sender_status.txt"

# The minimum length of a token to be considered
min_token_len: 2

 

