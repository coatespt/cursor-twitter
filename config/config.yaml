# Sample configuration for the Twitter subject detection pipeline

# So far the only input mode is RabbitMQ
mode: mqj
mq_host: localhost
mq_port: 5672
mq_queue: tweet_in

# The number of tweets in the rolling window. Two million is only about seven minute
# of the Decahose this was recorded on.  A more realistic window would be maybe half
# hour to an hour.  Enought to keep up with the changes over the course of a day.
# The idea is to keep it short enough to reflect changes but long enough to have 
# all the reasonably common word show up. 
window: 2000000

# Number of token files to keep before using the oldest to age tokens out of the 
# the window. At steady state, every time a token file is written, the oldest is 
# deleted. 
# The number of tokens in a file is calculated as: window / token_persist_files
token_persist_files: 20

# Frequency class rebuild interval (in token files)
# Rebuild the frequency classes every this many files 
# The frequency classes, therefore, can be build more often than the size
# of the window. 
rebuild_every_files: 10

# The number of tweets to process in each busy-word process cycle
batch: 15000

# Whether to log to the console
verbose: true 

# The directory to log to
log_dir: /Users/petercoates/python-work/cursor-twitter/logs

# The minimum Z score for a 3pk element to be considered anomalous
# I think this is typically a very high value--in two digits. AFAIK the 
# Distributionis not necessarily Gaussian.
# TODO: there may be a better way to estimate weirdness.
z_score: 3.5

# The number of frequency classes to create
freq_classes: 7

# The length of the busy word processor counter arrays
# This quantity cubed is the size of the 3pk space. 1000^3 = 1 billion.
# 
bw_array_len: 1000

# List of frequency classes (1-based) to skip for busy word detection. E.g., [1, 2] skips the most frequent and second most frequent classes.
skip_frequency_classes: []

# Busyword classes to consider in clustering.
# There is probably no point in using words from the most frequent classes.
# 
busyword_classes: [2,3,4,5,6]

# Word filtering configuration
filter:
  # Enable word filtering
  enabled: true
  # Path to file containing words to filter out
  filter_file: "/Users/petercoates/python-work/cursor-twitter/config/filter_lists/test_filters.txt"

# Persistence configuration
persistence:
  # Directory for storing state files (counts, filters, 3pk mappings)
  state_dir: "/Users/petercoates/python-work/data/state"

# Sender status tracking
sender:
  # File to track the last processed CSV file
  # NOTE: To future me--this is relative to where the sender runs from so 
  # it's actually next to the data/state directory. Confusing.
  status_file: "/Users/petercoates/python-work/data/sender/sender_status.txt"

# The minimum length of a token to be considered
min_token_len: 2

# Token filtering configuration
token_filters:
  # Enable token filtering
  enabled: true
  # Maximum token length (0 = no limit)
  max_length: 20
  # Minimum character diversity for long tokens (0.0-1.0)
  min_character_diversity: 0.3
  # Only apply diversity filter to tokens >= this length
  min_character_diversity_lower_limit: 8
  # Maximum ratio of consecutive repeated characters (0.0-1.0)
  max_character_repetition: 0.5
  # Maximum ratio of case alternations (0.0-1.0)
  max_case_alternations: 0.4
  # Maximum ratio of digits in token (0.0-1.0)
  max_number_letter_mix: 0.3
  # Reject hashtags (tokens starting with #)
  reject_hashtags: true
  # Reject URLs (tokens starting with http or www)
  reject_urls: true
  # Reject all-caps tokens above a certain length
  reject_all_caps_long: true
  # Only reject all-caps if token length >= this value
  all_caps_lower_limit: 5
  remove_urls: true

