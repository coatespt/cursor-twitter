# Sample configuration for the Twitter subject detection pipeline

# So far the only input mode is RabbitMQ
mode: mqj
mq_host: localhost
mq_port: 5672
mq_queue: tweet_in

# The number of tweets to keep in memory
# Also the set of tweets over which global frequency classes are computed
window: 2000000

# The number of tweets to process in each busy-word process ccle
batch: 15000

# Number of token files to keep before deleting oldest
# Token batch size is calculated as: window / token_persist_files
token_persist_files: 20

# Frequency class rebuild interval (in token files)
# Rebuild frequency classes every N token files
# Example: with 20 files and rebuild_every_files=10, rebuild every 500K tweets
rebuild_every_files: 10

# Whether to log to the console
verbose: true 

# The directory to log to
log_dir: logs

# The minimum Z score for a 3pk element to be considered anomalous
z_score: 3.5

# The number of frequency classes to create
freq_classes: 7

# The length of the busy word processor counter arrays
bw_array_len: 1000

# List of frequency classes (1-based) to skip for busy word detection. E.g., [1, 2] skips the most frequent and second most frequent classes.
skip_frequency_classes: [0,1]

# Word filtering configuration
filter:
  # Enable word filtering
  enabled: true
  # Path to file containing words to filter out
  filter_file: "config/filter_lists/test_filters.txt"

# Persistence configuration
persistence:
  # Directory for storing state files (counts, filters, 3pk mappings)
  state_dir: "data/state"

# Sender status tracking
sender:
  # File to track the last processed CSV file
  # NOTE: To future me--this is relative to where the sender runs from so 
  # it's actually next to the data/state directory. Confusing.
  status_file: "data/sender_status.txt"

# The minimum length of a token to be considered
min_token_len: 2

# Token filtering configuration
token_filters:
  # Enable token filtering
  enabled: true
  # Maximum token length (0 = no limit)
  max_length: 20
  # Minimum character diversity for long tokens (0.0-1.0)
  min_character_diversity: 0.3
  # Only apply diversity filter to tokens >= this length
  min_character_diversity_lower_limit: 8
  # Maximum ratio of consecutive repeated characters (0.0-1.0)
  max_character_repetition: 0.5
  # Maximum ratio of case alternations (0.0-1.0)
  max_case_alternations: 0.4
  # Maximum ratio of digits in token (0.0-1.0)
  max_number_letter_mix: 0.3
  # Reject hashtags (tokens starting with #)
  reject_hashtags: true
  # Reject URLs (tokens starting with http or www)
  reject_urls: true
  # Reject all-caps tokens above a certain length
  reject_all_caps_long: true
  # Only reject all-caps if token length >= this value
  all_caps_lower_limit: 5
  remove_urls: true

